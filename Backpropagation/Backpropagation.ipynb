{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question4_ECE657.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr62doUEST5G"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from numpy import savetxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y34X74qAbTRS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeN87Oidw-dD"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "class NeuralNetwork:\n",
        "\n",
        " def __init__(self, layers, alpha=0.1):\n",
        "    # initialize the list of weights matrices, then store the\n",
        "    # network architecture and learning rate\n",
        "    self.W = []\n",
        "    self.layers = layers\n",
        "    self.alpha = alpha\n",
        "    # start looping from the index of the first layer but\n",
        "    # stop before we reach the last two layers\n",
        "    for i in np.arange(0, len(layers) - 2):\n",
        "    \n",
        "      # randomly initialize a weight matrix connecting the\n",
        "      # number of nodes in each respective layer together,\n",
        "      # adding an extra node for the bias\n",
        "      w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "      self.W.append(w / np.sqrt(layers[i]))\n",
        "  \n",
        "\n",
        "    # the last two layers are a special case where the input\n",
        "    # connections need a bias term but the output does not\n",
        "    w = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "    self.W.append(w / np.sqrt(layers[-2]))\n",
        "    \n",
        " def __repr__(self):\n",
        "\t\t# construct and return a string that represents the network\n",
        "\t\t# architecture\n",
        "\t\treturn \"NeuralNetwork: {}\".format(\n",
        "\t\t\t\"-\".join(str(l) for l in self.layers))\n",
        "  \n",
        " def sigmoid(self, x):\n",
        "\t\treturn 1.0 / (1 + np.exp(-x))\n",
        "  \n",
        " def sigmoid_deriv(self, x):\n",
        "    return x * (1 - x)\n",
        "\n",
        " def propagate(self, X, y, epochs=1000, Update=100):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point and train\n",
        "\t\t\t# our network on it\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\tself.backpropagation(x, target)\n",
        "    \n",
        "\t\t\t# check to see if we should display a training update\n",
        "\t\t\tif epoch == 0 or (epoch + 1) % Update == 0:\n",
        "\t\t\t\tloss = self.calculate_loss(X, y)\n",
        "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
        "  \n",
        " def backpropagation(self, x, y):\n",
        "\t\t# constructed a list of output activations for each layer\n",
        "\t\t# as our data point flows through the network; the first\n",
        "\t\t# activation is a special case;it's just the input\n",
        "\t\t# feature vector itself\n",
        "\t\tA = [np.atleast_2d(x)]\n",
        "\n",
        "\t\t# loop over the layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# feedforward the activation at the current layer by\n",
        "\t\t\t# taking the dot product between the activation and\n",
        "\t\t\t# the weight matrix; this is called the \"net_linear input\"\n",
        "\t\t\t# to the current layer\n",
        "\t\t\tnet_linear = A[layer].dot(self.W[layer])\n",
        "\t\t\t# computing the \"net Non_linear output\" is simply applying our\n",
        "\t\t\t# nonlinear activation function to the net_linear input\n",
        "\t\t\tnon_linear_out = self.sigmoid(net_linear)\n",
        "\t\t\t# once we have the net output, add it to our list of\n",
        "\t\t\t# activations\n",
        "\t\t\tA.append(non_linear_out)\n",
        "   \n",
        "    \t\t\n",
        "\t\t# the first phase of backpropagation is to compute the\n",
        "\t\t# difference between our *prediction*  and the true target\n",
        "\t\t# value\n",
        "\t\terror = A[-1] - y\n",
        "\n",
        "\t\t# apply the chain rule and build our\n",
        "\t\t# list of deltas 'D'\n",
        "\t\tError_array = [error * self.sigmoid_deriv(A[-1])]\n",
        " \n",
        "\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n",
        "\t\t\tdelta = Error_array[-1].dot(self.W[layer].T)\n",
        "\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n",
        "\t\t\tError_array.append(delta)\n",
        "    # since we looped over our layers in reverse order we need to\n",
        "\t\t# reverse the deltas\n",
        "\t\tError_array = Error_array[::-1]\n",
        "\n",
        "\t\t# loop over the layers\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# update our weights by taking the dot product of the layer\n",
        "\t\t\t# activations with their respective deltas, then multiplying\n",
        "\t\t\t# this value by some small learning rate and adding to our\n",
        "\t\t\t# weight matrix \n",
        "\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(Error_array[layer])\n",
        "\n",
        " def predict(self, X, addBias=True):\n",
        "\t\tp = np.atleast_2d(X)\n",
        "  \n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
        "\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\n",
        "\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "\t\treturn p\n",
        "\n",
        " def calculate_loss(self, X, targets):\n",
        "\t\t# make predictions for the input data points then compute\n",
        "\t\t# the loss\n",
        "\t\ttargets = np.atleast_2d(targets)\n",
        "\t\tpredictions = self.predict(X, addBias=False)\n",
        "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\t\t# return the loss\n",
        "\t\treturn loss\n",
        "\n",
        " def save(self):\n",
        "    np.save('weights.npy', self.W)\n",
        "    pass\n",
        "  \n",
        "  \n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MmWWWmNTzFj"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQOIsjMVv4-"
      },
      "source": [
        "# Converting the files to numpy array\n",
        "train_dataset = genfromtxt('/content/drive/MyDrive/ECE657/train_data.csv', delimiter=',') # Replace this with your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3yj2BNWUXu3"
      },
      "source": [
        "# Converting the files to numpy array\n",
        "train_label = genfromtxt('/content/drive/MyDrive/ECE657/train_labels.csv', delimiter=',') # Replace this with your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Ry1E8GWkRc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IXzgxpzWnM0"
      },
      "source": [
        "# Obtained the sum of each rows\n",
        "sum_of_rows = train_dataset.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TgHmVf9W86S"
      },
      "source": [
        "# Normalized each row using their sums\n",
        "norm_train_dataset = train_dataset / sum_of_rows[:, np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mMPiYKqXM2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp1obzhiXSa4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxERYW8LXh4K"
      },
      "source": [
        "# Here our goal is to split the data into train data and validation data \n",
        "\n",
        "def split_data(X, y):\n",
        "    #Created a dummy array with the size of the dataset \n",
        "    arr_rand = np.random.rand(X.shape[0])\n",
        "\n",
        "    #This creates a randomly populated array of boolean variables the length \n",
        "    # of the arr_rand\n",
        "    split = arr_rand < np.percentile(arr_rand, 70)\n",
        "    #Here the spliting for test and train is carried out using the \n",
        "    #array of boolean variables \n",
        "    X_train = X[split]  \n",
        "    y_train = y[split]\n",
        "    X_test =  X[~split]\n",
        "    y_test =  y[~split]\n",
        "\n",
        "    # print(f\"{len(X_train)}, {len(y_train)}, {len(X_test)}, {len(y_test)}\")\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcNTFgv9aI98"
      },
      "source": [
        "x_train, y_train, X_test , y_test = split_data(norm_train_dataset, train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHEj6XLbggp"
      },
      "source": [
        "# savetxt('test_X.csv', X_test, delimiter=',')\n",
        "# savetxt('test_y.csv', y_test, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBvRI6FwbcYo"
      },
      "source": [
        "# Initialization of the neural network\n",
        "nn = NeuralNetwork([x_train.shape[1], 50 , 4])\n",
        "print(\"[BIO] {}\".format(nn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjYCvSvNOolN"
      },
      "source": [
        "nn.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSdhhhBGb6hV"
      },
      "source": [
        "nn.propagate(x_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh1iAGlk00jC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kI_buiP0TBK"
      },
      "source": [
        "# # evaluate the network\n",
        "# print(\"[LOG] Network Evaluation\")\n",
        "# predictions = nn.predict(X_test)\n",
        "# predictions = predictions.argmax(axis=1)\n",
        "# print(classification_report(y_test.argmax(axis=1), predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ZvU0Ga5Bsd"
      },
      "source": [
        "# len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6E5cVB815oG"
      },
      "source": [
        "# now that our network is trained, loop over the XOR data points\n",
        "prediction = []\n",
        "actual = []\n",
        "for (x , target) in zip(X_test , y_test):\n",
        "  # make a prediction on the data point \n",
        "  pred = nn.predict(x)\n",
        "  prediction.append(np.argmax(pred,axis=1))\n",
        "  actual.append(np.argmax(target,axis = 0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FVgCGW4SRM"
      },
      "source": [
        "correct = 0\n",
        "for i in range(0 , len(actual)):\n",
        "  # comparing individual prediction with the target \n",
        "  \n",
        "  if actual[i] == prediction[i]:\n",
        "    correct+=1\n",
        "# we then used the value obtained to calculate the accuracy\n",
        "print(f\"{int((correct/len(actual)) * 100)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYeFx_d5AXs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}